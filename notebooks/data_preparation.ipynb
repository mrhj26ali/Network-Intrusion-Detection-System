{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfb542ee-9ec5-4a28-bcec-7379e2347099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Environment initialized\n",
      "[INFO] Project root: C:\\Users\\DELL\\Desktop\\Network Intrusion Detection System\n",
      "[INFO] Raw data path: C:\\Users\\DELL\\Desktop\\Network Intrusion Detection System\\data\\raw\n",
      "[INFO] Processed data path: C:\\Users\\DELL\\Desktop\\Network Intrusion Detection System\\data\\processed\n",
      "[INFO] CSV chunk size: 250000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Notebook 1: Data Processing for Multi-Stage NIDS\n",
    "# Step 1: Environment Setup and Canonical Path Definitions\n",
    "# ============================================================\n",
    "\n",
    "# -----------------------------\n",
    "# Standard library imports\n",
    "# -----------------------------\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import gc\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "# -----------------------------\n",
    "# Third-party imports\n",
    "# -----------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# -----------------------------\n",
    "# Global configuration\n",
    "# -----------------------------\n",
    "\n",
    "# Silence non-critical warnings to keep logs readable\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Chunk size for streaming large CSV files (tunable)\n",
    "CSV_CHUNK_SIZE = 250_000\n",
    "\n",
    "# -----------------------------\n",
    "# Resolve project root directory\n",
    "# Assumption:\n",
    "#   project/\n",
    "#     ├── data/\n",
    "#     │    ├── raw/\n",
    "#     │    └── processed/\n",
    "#     ├── notebooks/\n",
    "#     └── venv/\n",
    "# -----------------------------\n",
    "\n",
    "CURRENT_DIR = os.getcwd()\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(CURRENT_DIR, \"..\"))\n",
    "\n",
    "# -----------------------------\n",
    "# Canonical data directories\n",
    "# -----------------------------\n",
    "\n",
    "RAW_DATA_DIR = os.path.join(PROJECT_ROOT, \"data\", \"raw\")\n",
    "PROCESSED_DATA_DIR = os.path.join(PROJECT_ROOT, \"data\", \"processed\")\n",
    "\n",
    "FEATURE_CHUNKS_DIR = os.path.join(PROCESSED_DATA_DIR, \"feature_chunks\")\n",
    "LABELS_DIR = os.path.join(PROCESSED_DATA_DIR, \"labels\")\n",
    "METADATA_DIR = os.path.join(PROCESSED_DATA_DIR, \"metadata\")\n",
    "\n",
    "# -----------------------------\n",
    "# Create processed directories if they do not exist\n",
    "# -----------------------------\n",
    "\n",
    "os.makedirs(FEATURE_CHUNKS_DIR, exist_ok=True)\n",
    "os.makedirs(LABELS_DIR, exist_ok=True)\n",
    "os.makedirs(METADATA_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Defensive sanity checks\n",
    "# -----------------------------\n",
    "\n",
    "assert os.path.isdir(RAW_DATA_DIR), \"Raw data directory does not exist\"\n",
    "assert os.path.isdir(PROCESSED_DATA_DIR), \"Processed data directory missing\"\n",
    "\n",
    "# -----------------------------\n",
    "# Logging helper (simple and explicit)\n",
    "# -----------------------------\n",
    "\n",
    "def log(msg):\n",
    "    print(msg)\n",
    "\n",
    "log(\"[OK] Environment initialized\")\n",
    "log(f\"[INFO] Project root: {PROJECT_ROOT}\")\n",
    "log(f\"[INFO] Raw data path: {RAW_DATA_DIR}\")\n",
    "log(f\"[INFO] Processed data path: {PROCESSED_DATA_DIR}\")\n",
    "log(f\"[INFO] CSV chunk size: {CSV_CHUNK_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e5789c4-833d-4e5b-b822-c4f77ea9b63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 1 raw CSV files\n",
      "  - TII-SSRC-23.csv\n",
      "[INFO] Inspecting schema from: TII-SSRC-23.csv\n",
      "[INFO] Number of columns: 86\n",
      "[INFO] Column names:\n",
      "  - Flow ID\n",
      "  - Src IP\n",
      "  - Src Port\n",
      "  - Dst IP\n",
      "  - Dst Port\n",
      "  - Protocol\n",
      "  - Timestamp\n",
      "  - Flow Duration\n",
      "  - Total Fwd Packet\n",
      "  - Total Bwd packets\n",
      "  - Total Length of Fwd Packet\n",
      "  - Total Length of Bwd Packet\n",
      "  - Fwd Packet Length Max\n",
      "  - Fwd Packet Length Min\n",
      "  - Fwd Packet Length Mean\n",
      "  - Fwd Packet Length Std\n",
      "  - Bwd Packet Length Max\n",
      "  - Bwd Packet Length Min\n",
      "  - Bwd Packet Length Mean\n",
      "  - Bwd Packet Length Std\n",
      "  - Flow Bytes/s\n",
      "  - Flow Packets/s\n",
      "  - Flow IAT Mean\n",
      "  - Flow IAT Std\n",
      "  - Flow IAT Max\n",
      "  - Flow IAT Min\n",
      "  - Fwd IAT Total\n",
      "  - Fwd IAT Mean\n",
      "  - Fwd IAT Std\n",
      "  - Fwd IAT Max\n",
      "  - Fwd IAT Min\n",
      "  - Bwd IAT Total\n",
      "  - Bwd IAT Mean\n",
      "  - Bwd IAT Std\n",
      "  - Bwd IAT Max\n",
      "  - Bwd IAT Min\n",
      "  - Fwd PSH Flags\n",
      "  - Bwd PSH Flags\n",
      "  - Fwd URG Flags\n",
      "  - Bwd URG Flags\n",
      "  - Fwd Header Length\n",
      "  - Bwd Header Length\n",
      "  - Fwd Packets/s\n",
      "  - Bwd Packets/s\n",
      "  - Packet Length Min\n",
      "  - Packet Length Max\n",
      "  - Packet Length Mean\n",
      "  - Packet Length Std\n",
      "  - Packet Length Variance\n",
      "  - FIN Flag Count\n",
      "  - SYN Flag Count\n",
      "  - RST Flag Count\n",
      "  - PSH Flag Count\n",
      "  - ACK Flag Count\n",
      "  - URG Flag Count\n",
      "  - CWR Flag Count\n",
      "  - ECE Flag Count\n",
      "  - Down/Up Ratio\n",
      "  - Average Packet Size\n",
      "  - Fwd Segment Size Avg\n",
      "  - Bwd Segment Size Avg\n",
      "  - Fwd Bytes/Bulk Avg\n",
      "  - Fwd Packet/Bulk Avg\n",
      "  - Fwd Bulk Rate Avg\n",
      "  - Bwd Bytes/Bulk Avg\n",
      "  - Bwd Packet/Bulk Avg\n",
      "  - Bwd Bulk Rate Avg\n",
      "  - Subflow Fwd Packets\n",
      "  - Subflow Fwd Bytes\n",
      "  - Subflow Bwd Packets\n",
      "  - Subflow Bwd Bytes\n",
      "  - FWD Init Win Bytes\n",
      "  - Bwd Init Win Bytes\n",
      "  - Fwd Act Data Pkts\n",
      "  - Fwd Seg Size Min\n",
      "  - Active Mean\n",
      "  - Active Std\n",
      "  - Active Max\n",
      "  - Active Min\n",
      "  - Idle Mean\n",
      "  - Idle Std\n",
      "  - Idle Max\n",
      "  - Idle Min\n",
      "  - Label\n",
      "  - Traffic Type\n",
      "  - Traffic Subtype\n",
      "[OK] All identifier columns detected\n",
      "[OK] All raw label columns detected\n",
      "[DONE] Raw schema saved to metadata: C:\\Users\\DELL\\Desktop\\Network Intrusion Detection System\\data\\processed\\metadata\\raw_schema.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Step 2: Raw CSV Discovery and Schema Inspection\n",
    "# Purpose:\n",
    "#   - Identify available raw CSV files\n",
    "#   - Inspect column schema safely (no full load)\n",
    "#   - Validate presence of labels and identifiers\n",
    "#   - Establish authoritative column names for downstream steps\n",
    "# ============================================================\n",
    "\n",
    "# -----------------------------\n",
    "# Locate raw CSV files\n",
    "# -----------------------------\n",
    "\n",
    "raw_csv_files = [\n",
    "    f for f in os.listdir(RAW_DATA_DIR)\n",
    "    if f.lower().endswith(\".csv\")\n",
    "]\n",
    "\n",
    "assert len(raw_csv_files) > 0, \"No CSV files found in raw data directory\"\n",
    "\n",
    "log(f\"[INFO] Found {len(raw_csv_files)} raw CSV files\")\n",
    "\n",
    "# Print file names for traceability\n",
    "for fname in raw_csv_files:\n",
    "    log(f\"  - {fname}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Select one CSV for schema inspection\n",
    "# This does NOT assume uniform ordering across files\n",
    "# -----------------------------\n",
    "\n",
    "sample_csv_path = os.path.join(RAW_DATA_DIR, raw_csv_files[0])\n",
    "log(f\"[INFO] Inspecting schema from: {raw_csv_files[0]}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Read a very small sample (safe for memory)\n",
    "# -----------------------------\n",
    "\n",
    "sample_df = pd.read_csv(\n",
    "    sample_csv_path,\n",
    "    nrows=5\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Inspect basic structure\n",
    "# -----------------------------\n",
    "\n",
    "log(f\"[INFO] Number of columns: {sample_df.shape[1]}\")\n",
    "\n",
    "log(\"[INFO] Column names:\")\n",
    "for col in sample_df.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Check for critical columns\n",
    "# These checks are defensive, not destructive\n",
    "# -----------------------------\n",
    "\n",
    "EXPECTED_IDENTIFIER_COLUMNS = [\n",
    "    \"Flow ID\",\n",
    "    \"Src IP\",\n",
    "    \"Dst IP\",\n",
    "    \"Timestamp\"\n",
    "]\n",
    "\n",
    "EXPECTED_RAW_LABEL_COLUMNS = [\n",
    "    \"Label\",\n",
    "    \"Traffic Type\",\n",
    "    \"Traffic Subtype\"\n",
    "]\n",
    "\n",
    "missing_identifiers = [\n",
    "    c for c in EXPECTED_IDENTIFIER_COLUMNS\n",
    "    if c not in sample_df.columns\n",
    "]\n",
    "\n",
    "missing_labels = [\n",
    "    c for c in EXPECTED_RAW_LABEL_COLUMNS\n",
    "    if c not in sample_df.columns\n",
    "]\n",
    "\n",
    "if missing_identifiers:\n",
    "    log(\"[WARNING] Missing identifier columns:\")\n",
    "    for c in missing_identifiers:\n",
    "        log(f\"  - {c}\")\n",
    "else:\n",
    "    log(\"[OK] All identifier columns detected\")\n",
    "\n",
    "if missing_labels:\n",
    "    log(\"[WARNING] Missing raw label columns:\")\n",
    "    for c in missing_labels:\n",
    "        log(f\"  - {c}\")\n",
    "else:\n",
    "    log(\"[OK] All raw label columns detected\")\n",
    "\n",
    "# -----------------------------\n",
    "# Persist schema metadata\n",
    "# This locks column order and names for the entire project\n",
    "# -----------------------------\n",
    "\n",
    "schema_metadata = {\n",
    "    \"raw_columns\": list(sample_df.columns),\n",
    "    \"identifier_columns\": EXPECTED_IDENTIFIER_COLUMNS,\n",
    "    \"raw_label_columns\": EXPECTED_RAW_LABEL_COLUMNS,\n",
    "    \"num_columns\": sample_df.shape[1]\n",
    "}\n",
    "\n",
    "schema_metadata_path = os.path.join(\n",
    "    METADATA_DIR, \"raw_schema.json\"\n",
    ")\n",
    "\n",
    "with open(schema_metadata_path, \"w\") as f:\n",
    "    json.dump(schema_metadata, f, indent=2)\n",
    "\n",
    "log(f\"[DONE] Raw schema saved to metadata: {schema_metadata_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Cleanup\n",
    "# -----------------------------\n",
    "\n",
    "del sample_df\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db19570-4e80-4183-b79c-7b9c23332461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Raw chunks output directory: C:\\Users\\DELL\\Desktop\\Network Intrusion Detection System\\data\\processed\\raw_chunks\n",
      "[INFO] Starting streaming ingestion from: TII-SSRC-23.csv\n",
      "[INFO] Processed 5 chunks...\n",
      "[INFO] Processed 10 chunks...\n",
      "[INFO] Processed 15 chunks...\n",
      "[INFO] Processed 20 chunks...\n",
      "[INFO] Processed 25 chunks...\n",
      "[INFO] Processed 30 chunks...\n",
      "[INFO] Processed 35 chunks...\n",
      "[DONE] Raw CSV ingestion completed\n",
      "[INFO] Total chunks written: 35\n",
      "[INFO] Total rows processed: 8,656,767\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Step 3: Streaming CSV Ingestion and Chunked Parquet Conversion\n",
    "# Purpose:\n",
    "#   - Read the large raw CSV safely in chunks\n",
    "#   - Persist chunks as Parquet files\n",
    "#   - Preserve raw data exactly (no cleaning, no labeling)\n",
    "#   - Enable scalable downstream processing\n",
    "# ============================================================\n",
    "\n",
    "# -----------------------------\n",
    "# Output directory for raw chunks\n",
    "# -----------------------------\n",
    "\n",
    "RAW_CHUNKS_DIR = os.path.join(PROCESSED_DATA_DIR, \"raw_chunks\")\n",
    "os.makedirs(RAW_CHUNKS_DIR, exist_ok=True)\n",
    "\n",
    "log(f\"[INFO] Raw chunks output directory: {RAW_CHUNKS_DIR}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load authoritative schema\n",
    "# -----------------------------\n",
    "\n",
    "schema_path = os.path.join(METADATA_DIR, \"raw_schema.json\")\n",
    "with open(schema_path, \"r\") as f:\n",
    "    raw_schema = json.load(f)\n",
    "\n",
    "expected_columns = raw_schema[\"raw_columns\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Initialize streaming read\n",
    "# -----------------------------\n",
    "\n",
    "chunk_index = 0\n",
    "total_rows = 0\n",
    "\n",
    "csv_path = os.path.join(RAW_DATA_DIR, raw_csv_files[0])\n",
    "\n",
    "log(f\"[INFO] Starting streaming ingestion from: {raw_csv_files[0]}\")\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    csv_path,\n",
    "    chunksize=CSV_CHUNK_SIZE,\n",
    "    low_memory=False\n",
    "):\n",
    "    # -------------------------\n",
    "    # Enforce column order\n",
    "    # -------------------------\n",
    "    chunk = chunk[expected_columns]\n",
    "\n",
    "    # -------------------------\n",
    "    # Convert to Parquet\n",
    "    # -------------------------\n",
    "    chunk_path = os.path.join(\n",
    "        RAW_CHUNKS_DIR,\n",
    "        f\"raw_chunk_{chunk_index:03d}.parquet\"\n",
    "    )\n",
    "\n",
    "    table = pa.Table.from_pandas(chunk, preserve_index=False)\n",
    "    pq.write_table(table, chunk_path)\n",
    "\n",
    "    rows_in_chunk = len(chunk)\n",
    "    total_rows += rows_in_chunk\n",
    "\n",
    "    # -------------------------\n",
    "    # Progress logging\n",
    "    # -------------------------\n",
    "    if (chunk_index + 1) % 5 == 0:\n",
    "        log(f\"[INFO] Processed {chunk_index + 1} chunks...\")\n",
    "\n",
    "    chunk_index += 1\n",
    "\n",
    "    # -------------------------\n",
    "    # Memory cleanup\n",
    "    # -------------------------\n",
    "    del chunk, table\n",
    "    gc.collect()\n",
    "\n",
    "# -----------------------------\n",
    "# Persist ingestion metadata\n",
    "# -----------------------------\n",
    "\n",
    "ingestion_metadata = {\n",
    "    \"source_csv\": raw_csv_files[0],\n",
    "    \"chunks_written\": chunk_index,\n",
    "    \"rows_processed\": total_rows,\n",
    "    \"chunk_size\": CSV_CHUNK_SIZE\n",
    "}\n",
    "\n",
    "ingestion_metadata_path = os.path.join(\n",
    "    METADATA_DIR, \"raw_ingestion_stats.json\"\n",
    ")\n",
    "\n",
    "with open(ingestion_metadata_path, \"w\") as f:\n",
    "    json.dump(ingestion_metadata, f, indent=2)\n",
    "\n",
    "log(\"[DONE] Raw CSV ingestion completed\")\n",
    "log(f\"[INFO] Total chunks written: {chunk_index}\")\n",
    "log(f\"[INFO] Total rows processed: {total_rows:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c99e1c4-34de-4b71-82eb-163aefc9df21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Labeled chunks output directory: C:\\Users\\DELL\\Desktop\\Network Intrusion Detection System\\data\\processed\\labeled_chunks\n",
      "[INFO] Labeled 5 chunks...\n",
      "[INFO] Labeled 10 chunks...\n",
      "[INFO] Labeled 15 chunks...\n",
      "[INFO] Labeled 20 chunks...\n",
      "[INFO] Labeled 25 chunks...\n",
      "[INFO] Labeled 30 chunks...\n",
      "[INFO] Labeled 35 chunks...\n",
      "[DONE] Label construction completed\n",
      "[INFO] Total chunks written: 35\n",
      "[INFO] Total rows processed: 8,656,767\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Step 4: Label Normalization and Stage-wise Label Construction\n",
    "# Purpose:\n",
    "#   - Construct clean labels for the 3-stage IDS architecture\n",
    "#   - Do NOT modify or touch feature columns\n",
    "#   - Operate chunk-wise to preserve scalability\n",
    "#\n",
    "# Stage 1:\n",
    "#   Binary classification\n",
    "#   0 -> Benign\n",
    "#   1 -> Malicious\n",
    "#\n",
    "# Stage 2:\n",
    "#   Attack family classification + Non-Attack\n",
    "#\n",
    "# Stage 3:\n",
    "#   Attack subtype (fine-grained)\n",
    "# ============================================================\n",
    "\n",
    "# -----------------------------\n",
    "# Input / Output directories\n",
    "# -----------------------------\n",
    "\n",
    "RAW_CHUNKS_DIR = os.path.join(PROCESSED_DATA_DIR, \"raw_chunks\")\n",
    "LABELED_CHUNKS_DIR = os.path.join(PROCESSED_DATA_DIR, \"labeled_chunks\")\n",
    "\n",
    "os.makedirs(LABELED_CHUNKS_DIR, exist_ok=True)\n",
    "\n",
    "log(f\"[INFO] Labeled chunks output directory: {LABELED_CHUNKS_DIR}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Label column names\n",
    "# -----------------------------\n",
    "\n",
    "RAW_LABEL_STAGE_1 = \"Label\"\n",
    "RAW_LABEL_STAGE_2 = \"Traffic Type\"\n",
    "RAW_LABEL_STAGE_3 = \"Traffic Subtype\"\n",
    "\n",
    "DERIVED_LABEL_STAGE_1 = \"label_stage1\"\n",
    "DERIVED_LABEL_STAGE_2 = \"label_stage2\"\n",
    "DERIVED_LABEL_STAGE_3 = \"label_stage3\"\n",
    "\n",
    "# -----------------------------\n",
    "# Stage 1 label mapping\n",
    "# -----------------------------\n",
    "\n",
    "def build_stage1_label(series):\n",
    "    # Benign -> 0, everything else -> 1\n",
    "    return (series != \"Benign\").astype(\"int8\")\n",
    "\n",
    "# -----------------------------\n",
    "# Stage 2 label mapping\n",
    "# -----------------------------\n",
    "\n",
    "def build_stage2_label(series):\n",
    "    # Normalize text and preserve Non-Attack explicitly\n",
    "    return series.fillna(\"Unknown\").astype(str)\n",
    "\n",
    "# -----------------------------\n",
    "# Stage 3 label mapping\n",
    "# -----------------------------\n",
    "\n",
    "def build_stage3_label(series):\n",
    "    # Preserve subtype as-is (fine-grained)\n",
    "    return series.fillna(\"Unknown\").astype(str)\n",
    "\n",
    "# -----------------------------\n",
    "# Process chunks\n",
    "# -----------------------------\n",
    "\n",
    "chunk_count = 0\n",
    "total_rows = 0\n",
    "\n",
    "for fname in sorted(os.listdir(RAW_CHUNKS_DIR)):\n",
    "    input_path = os.path.join(RAW_CHUNKS_DIR, fname)\n",
    "\n",
    "    df = pq.read_table(input_path).to_pandas()\n",
    "\n",
    "    # -------------------------\n",
    "    # Derived label construction\n",
    "    # -------------------------\n",
    "\n",
    "    df[DERIVED_LABEL_STAGE_1] = build_stage1_label(df[RAW_LABEL_STAGE_1])\n",
    "    df[DERIVED_LABEL_STAGE_2] = build_stage2_label(df[RAW_LABEL_STAGE_2])\n",
    "    df[DERIVED_LABEL_STAGE_3] = build_stage3_label(df[RAW_LABEL_STAGE_3])\n",
    "\n",
    "    # -------------------------\n",
    "    # Persist labeled chunk\n",
    "    # -------------------------\n",
    "\n",
    "    output_path = os.path.join(\n",
    "        LABELED_CHUNKS_DIR,\n",
    "        fname.replace(\"raw_chunk\", \"labeled_chunk\")\n",
    "    )\n",
    "\n",
    "    table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "    pq.write_table(table, output_path)\n",
    "\n",
    "    rows_in_chunk = len(df)\n",
    "    total_rows += rows_in_chunk\n",
    "    chunk_count += 1\n",
    "\n",
    "    if chunk_count % 5 == 0:\n",
    "        log(f\"[INFO] Labeled {chunk_count} chunks...\")\n",
    "\n",
    "    del df, table\n",
    "    gc.collect()\n",
    "\n",
    "# -----------------------------\n",
    "# Persist labeling metadata\n",
    "# -----------------------------\n",
    "\n",
    "labeling_metadata = {\n",
    "    \"chunks_written\": chunk_count,\n",
    "    \"rows_processed\": total_rows,\n",
    "    \"stage1_definition\": \"Benign=0, Non-Benign=1\",\n",
    "    \"stage2_definition\": \"Traffic Type\",\n",
    "    \"stage3_definition\": \"Traffic Subtype\"\n",
    "}\n",
    "\n",
    "labeling_metadata_path = os.path.join(\n",
    "    METADATA_DIR, \"labeling_stats.json\"\n",
    ")\n",
    "\n",
    "with open(labeling_metadata_path, \"w\") as f:\n",
    "    json.dump(labeling_metadata, f, indent=2)\n",
    "\n",
    "log(\"[DONE] Label construction completed\")\n",
    "log(f\"[INFO] Total chunks written: {chunk_count}\")\n",
    "log(f\"[INFO] Total rows processed: {total_rows:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14df3349-c3ad-4651-bb1f-53cbf1039ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Feature-only chunks output directory: C:\\Users\\DELL\\Desktop\\Network Intrusion Detection System\\data\\processed\\feature_chunks\n",
      "[INFO] Extracted features from 5 chunks...\n",
      "[INFO] Extracted features from 10 chunks...\n",
      "[INFO] Extracted features from 15 chunks...\n",
      "[INFO] Extracted features from 20 chunks...\n",
      "[INFO] Extracted features from 25 chunks...\n",
      "[INFO] Extracted features from 30 chunks...\n",
      "[INFO] Extracted features from 35 chunks...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feature_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 108\u001b[39m\n\u001b[32m     99\u001b[39m     gc.collect()\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# Persist feature metadata\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    105\u001b[39m feature_metadata = {\n\u001b[32m    106\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mchunks_written\u001b[39m\u001b[33m\"\u001b[39m: chunk_count,\n\u001b[32m    107\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrows_processed\u001b[39m\u001b[33m\"\u001b[39m: total_rows,\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnum_features\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mfeature_df\u001b[49m.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    109\u001b[39m }\n\u001b[32m    111\u001b[39m feature_metadata_path = os.path.join(\n\u001b[32m    112\u001b[39m     METADATA_DIR, \u001b[33m\"\u001b[39m\u001b[33mfeature_schema.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    113\u001b[39m )\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(feature_metadata_path, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mNameError\u001b[39m: name 'feature_df' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Step 5: Column Role Locking and Feature-Only Dataset Creation\n",
    "# Purpose:\n",
    "#   - Explicitly separate features from identifiers and labels\n",
    "#   - Remove all non-feature columns to prevent leakage\n",
    "#   - Produce feature-only chunks suitable for all stages\n",
    "#\n",
    "# This step ensures:\n",
    "#   - No identifiers leak contextual information\n",
    "#   - No raw or derived labels are seen by models\n",
    "#   - Feature space is identical for Stage 1, 2, and 3\n",
    "# ============================================================\n",
    "\n",
    "# -----------------------------\n",
    "# Directory definitions\n",
    "# -----------------------------\n",
    "\n",
    "LABELED_CHUNKS_DIR = os.path.join(PROCESSED_DATA_DIR, \"labeled_chunks\")\n",
    "FEATURE_CHUNKS_DIR = os.path.join(PROCESSED_DATA_DIR, \"feature_chunks\")\n",
    "\n",
    "os.makedirs(FEATURE_CHUNKS_DIR, exist_ok=True)\n",
    "\n",
    "log(f\"[INFO] Feature-only chunks output directory: {FEATURE_CHUNKS_DIR}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Column role definitions\n",
    "# -----------------------------\n",
    "\n",
    "IDENTIFIER_COLUMNS = [\n",
    "    \"Flow ID\",\n",
    "    \"Src IP\",\n",
    "    \"Dst IP\",\n",
    "    \"Timestamp\"\n",
    "]\n",
    "\n",
    "RAW_LABEL_COLUMNS = [\n",
    "    \"Label\",\n",
    "    \"Traffic Type\",\n",
    "    \"Traffic Subtype\"\n",
    "]\n",
    "\n",
    "DERIVED_LABEL_COLUMNS = [\n",
    "    \"label_stage1\",\n",
    "    \"label_stage2\",\n",
    "    \"label_stage3\"\n",
    "]\n",
    "\n",
    "COLUMNS_TO_DROP = (\n",
    "    IDENTIFIER_COLUMNS\n",
    "    + RAW_LABEL_COLUMNS\n",
    "    + DERIVED_LABEL_COLUMNS\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Sanity check on schema\n",
    "# -----------------------------\n",
    "\n",
    "sample_chunk = pq.read_table(\n",
    "    os.path.join(LABELED_CHUNKS_DIR, os.listdir(LABELED_CHUNKS_DIR)[0])\n",
    ").to_pandas()\n",
    "\n",
    "missing_cols = set(COLUMNS_TO_DROP) - set(sample_chunk.columns)\n",
    "assert len(missing_cols) == 0, f\"Missing expected columns: {missing_cols}\"\n",
    "\n",
    "del sample_chunk\n",
    "gc.collect()\n",
    "\n",
    "# -----------------------------\n",
    "# Feature extraction\n",
    "# -----------------------------\n",
    "\n",
    "chunk_count = 0\n",
    "total_rows = 0\n",
    "\n",
    "for fname in sorted(os.listdir(LABELED_CHUNKS_DIR)):\n",
    "    input_path = os.path.join(LABELED_CHUNKS_DIR, fname)\n",
    "\n",
    "    df = pq.read_table(input_path).to_pandas()\n",
    "\n",
    "    # Drop all non-feature columns\n",
    "    feature_df = df.drop(columns=COLUMNS_TO_DROP)\n",
    "\n",
    "    output_path = os.path.join(\n",
    "        FEATURE_CHUNKS_DIR,\n",
    "        fname.replace(\"labeled_chunk\", \"feature_chunk\")\n",
    "    )\n",
    "\n",
    "    table = pa.Table.from_pandas(feature_df, preserve_index=False)\n",
    "    pq.write_table(table, output_path)\n",
    "\n",
    "    rows_in_chunk = len(feature_df)\n",
    "    total_rows += rows_in_chunk\n",
    "    chunk_count += 1\n",
    "\n",
    "    if chunk_count % 5 == 0:\n",
    "        log(f\"[INFO] Extracted features from {chunk_count} chunks...\")\n",
    "\n",
    "    del df, feature_df, table\n",
    "    gc.collect()\n",
    "\n",
    "# -----------------------------\n",
    "# Persist feature metadata\n",
    "# -----------------------------\n",
    "\n",
    "feature_metadata = {\n",
    "    \"chunks_written\": chunk_count,\n",
    "    \"rows_processed\": total_rows,\n",
    "    \"num_features\": feature_df.shape[1]\n",
    "}\n",
    "\n",
    "feature_metadata_path = os.path.join(\n",
    "    METADATA_DIR, \"feature_schema.json\"\n",
    ")\n",
    "\n",
    "with open(feature_metadata_path, \"w\") as f:\n",
    "    json.dump(feature_metadata, f, indent=2)\n",
    "\n",
    "log(\"[DONE] Feature-only dataset creation completed\")\n",
    "log(f\"[INFO] Total chunks written: {chunk_count}\")\n",
    "log(f\"[INFO] Total rows processed: {total_rows:,}\")\n",
    "log(f\"[INFO] Number of features per row: {feature_df.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad4ff206-469d-48b5-83f2-369f5468f1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Paths restored\n",
      "Processed data path: C:\\Users\\DELL\\Desktop\\Network Intrusion Detection System\\data\\processed\n",
      "[OK] Feature metadata written successfully\n",
      "[INFO] Number of features: 79\n"
     ]
    }
   ],
   "source": [
    "# Re-establish canonical project paths (kernel-safe)\n",
    "\n",
    "import os\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "PROCESSED_DATA_PATH = os.path.join(PROJECT_ROOT, \"data\", \"processed\")\n",
    "\n",
    "print(\"[OK] Paths restored\")\n",
    "print(\"Processed data path:\", PROCESSED_DATA_PATH)\n",
    "# Fix missing feature metadata after successful extraction\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "FEATURE_CHUNKS_DIR = os.path.join(PROCESSED_DATA_PATH, \"feature_chunks\")\n",
    "METADATA_DIR = os.path.join(PROCESSED_DATA_PATH, \"metadata\")\n",
    "os.makedirs(METADATA_DIR, exist_ok=True)\n",
    "\n",
    "sample_file = sorted(os.listdir(FEATURE_CHUNKS_DIR))[0]\n",
    "sample_df = pq.read_table(\n",
    "    os.path.join(FEATURE_CHUNKS_DIR, sample_file)\n",
    ").to_pandas()\n",
    "\n",
    "feature_metadata = {\n",
    "    \"chunks_written\": len(os.listdir(FEATURE_CHUNKS_DIR)),\n",
    "    \"rows_processed\": 8_656_767,\n",
    "    \"num_features\": sample_df.shape[1],\n",
    "    \"feature_names\": list(sample_df.columns)\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(METADATA_DIR, \"feature_schema.json\")\n",
    "with open(metadata_path, \"w\") as f:\n",
    "    json.dump(feature_metadata, f, indent=2)\n",
    "\n",
    "print(\"[OK] Feature metadata written successfully\")\n",
    "print(f\"[INFO] Number of features: {feature_metadata['num_features']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93c65c75-0f02-473c-aef9-98c91c66d66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "FINAL DATA PREPARATION DELIVERABLES – FULL INSPECTION\n",
      "==========================================================================================\n",
      "\n",
      "[1] RAW DATA CHUNKS\n",
      "Directory: C:\\Users\\DELL\\Desktop\\Network Intrusion Detection System\\data\\processed\\raw_chunks\n",
      "Number of chunks: 35\n",
      "Sample chunk: raw_chunk_000.parquet\n",
      "Rows in sample chunk: 250000\n",
      "Number of columns: 86\n",
      "\n",
      "[2] LABELED DATA CHUNKS (Stage labels included)\n",
      "Directory: C:\\Users\\DELL\\Desktop\\Network Intrusion Detection System\\data\\processed\\labeled_chunks\n",
      "Number of chunks: 35\n",
      "Sample chunk: labeled_chunk_000.parquet\n",
      "Rows in sample chunk: 250000\n",
      "Label columns: ['label_stage1', 'label_stage2', 'label_stage3']\n",
      "\n",
      "[3] FEATURE-ONLY CHUNKS (Leakage-safe)\n",
      "Directory: C:\\Users\\DELL\\Desktop\\Network Intrusion Detection System\\data\\processed\\feature_chunks\n",
      "Number of chunks: 35\n",
      "Sample chunk: feature_chunk_000.parquet\n",
      "Rows in sample chunk: 250000\n",
      "Number of features: 79\n",
      "\n",
      "[4] METADATA FILES\n",
      "Directory: C:\\Users\\DELL\\Desktop\\Network Intrusion Detection System\\data\\processed\\metadata\n",
      "\n",
      "--- feature_schema.json ---\n",
      "chunks_written: 35\n",
      "rows_processed: 8656767\n",
      "num_features: 79\n",
      "feature_names: ['Src Port', 'Dst Port', 'Protocol', 'Flow Duration', 'Total Fwd Packet', 'Total Bwd packets', 'Total Length of Fwd Packet', 'Total Length of Bwd Packet', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Packet Length Min', 'Packet Length Max', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'CWR Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Fwd Segment Size Avg', 'Bwd Segment Size Avg', 'Fwd Bytes/Bulk Avg', 'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg', 'Bwd Bytes/Bulk Avg', 'Bwd Packet/Bulk Avg', 'Bwd Bulk Rate Avg', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'FWD Init Win Bytes', 'Bwd Init Win Bytes', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min']\n",
      "\n",
      "--- labeling_stats.json ---\n",
      "chunks_written: 35\n",
      "rows_processed: 8656767\n",
      "stage1_definition: Benign=0, Non-Benign=1\n",
      "stage2_definition: Traffic Type\n",
      "stage3_definition: Traffic Subtype\n",
      "\n",
      "--- raw_ingestion_stats.json ---\n",
      "source_csv: TII-SSRC-23.csv\n",
      "chunks_written: 35\n",
      "rows_processed: 8656767\n",
      "chunk_size: 250000\n",
      "\n",
      "--- raw_schema.json ---\n",
      "raw_columns: ['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Total Fwd Packet', 'Total Bwd packets', 'Total Length of Fwd Packet', 'Total Length of Bwd Packet', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Packet Length Min', 'Packet Length Max', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'CWR Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Fwd Segment Size Avg', 'Bwd Segment Size Avg', 'Fwd Bytes/Bulk Avg', 'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg', 'Bwd Bytes/Bulk Avg', 'Bwd Packet/Bulk Avg', 'Bwd Bulk Rate Avg', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'FWD Init Win Bytes', 'Bwd Init Win Bytes', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label', 'Traffic Type', 'Traffic Subtype']\n",
      "identifier_columns: ['Flow ID', 'Src IP', 'Dst IP', 'Timestamp']\n",
      "raw_label_columns: ['Label', 'Traffic Type', 'Traffic Subtype']\n",
      "num_columns: 86\n",
      "\n",
      "==========================================================================================\n",
      "STATUS: DATA PREPARATION COMPLETED SUCCESSFULLY\n",
      "Artifacts are clean, structured, and ready for modeling stages.\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FINAL AUDIT & PRESENTATION CELL\n",
    "# 01_data_preparation.ipynb\n",
    "# This cell ONLY reads and displays delivered artifacts.\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"FINAL DATA PREPARATION DELIVERABLES – FULL INSPECTION\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Paths\n",
    "# ------------------------------------------------------------\n",
    "RAW_CHUNKS_DIR = os.path.join(PROCESSED_DATA_PATH, \"raw_chunks\")\n",
    "LABELED_CHUNKS_DIR = os.path.join(PROCESSED_DATA_PATH, \"labeled_chunks\")\n",
    "FEATURE_CHUNKS_DIR = os.path.join(PROCESSED_DATA_PATH, \"feature_chunks\")\n",
    "METADATA_DIR = os.path.join(PROCESSED_DATA_PATH, \"metadata\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helper functions\n",
    "# ------------------------------------------------------------\n",
    "def list_parquet_files(path):\n",
    "    return sorted([f for f in os.listdir(path) if f.endswith(\".parquet\")]) if os.path.exists(path) else []\n",
    "\n",
    "def inspect_parquet_schema(path, fname):\n",
    "    table = pq.read_table(os.path.join(path, fname))\n",
    "    return table.schema.names, table.num_rows\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Raw Chunks Inspection\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n[1] RAW DATA CHUNKS\")\n",
    "raw_files = list_parquet_files(RAW_CHUNKS_DIR)\n",
    "print(f\"Directory: {RAW_CHUNKS_DIR}\")\n",
    "print(f\"Number of chunks: {len(raw_files)}\")\n",
    "\n",
    "if raw_files:\n",
    "    cols, rows = inspect_parquet_schema(RAW_CHUNKS_DIR, raw_files[0])\n",
    "    print(f\"Sample chunk: {raw_files[0]}\")\n",
    "    print(f\"Rows in sample chunk: {rows}\")\n",
    "    print(f\"Number of columns: {len(cols)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Labeled Chunks Inspection\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n[2] LABELED DATA CHUNKS (Stage labels included)\")\n",
    "labeled_files = list_parquet_files(LABELED_CHUNKS_DIR)\n",
    "print(f\"Directory: {LABELED_CHUNKS_DIR}\")\n",
    "print(f\"Number of chunks: {len(labeled_files)}\")\n",
    "\n",
    "if labeled_files:\n",
    "    cols, rows = inspect_parquet_schema(LABELED_CHUNKS_DIR, labeled_files[0])\n",
    "    label_cols = [c for c in cols if c.startswith(\"label_\")]\n",
    "    print(f\"Sample chunk: {labeled_files[0]}\")\n",
    "    print(f\"Rows in sample chunk: {rows}\")\n",
    "    print(f\"Label columns: {label_cols}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Feature-only Chunks Inspection\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n[3] FEATURE-ONLY CHUNKS (Leakage-safe)\")\n",
    "feature_files = list_parquet_files(FEATURE_CHUNKS_DIR)\n",
    "print(f\"Directory: {FEATURE_CHUNKS_DIR}\")\n",
    "print(f\"Number of chunks: {len(feature_files)}\")\n",
    "\n",
    "if feature_files:\n",
    "    cols, rows = inspect_parquet_schema(FEATURE_CHUNKS_DIR, feature_files[0])\n",
    "    print(f\"Sample chunk: {feature_files[0]}\")\n",
    "    print(f\"Rows in sample chunk: {rows}\")\n",
    "    print(f\"Number of features: {len(cols)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Metadata Files Inspection\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n[4] METADATA FILES\")\n",
    "print(f\"Directory: {METADATA_DIR}\")\n",
    "\n",
    "if os.path.exists(METADATA_DIR):\n",
    "    for fname in sorted(os.listdir(METADATA_DIR)):\n",
    "        print(f\"\\n--- {fname} ---\")\n",
    "        with open(os.path.join(METADATA_DIR, fname), \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        for k, v in data.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"Metadata directory not found\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Final Status\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"STATUS: DATA PREPARATION COMPLETED SUCCESSFULLY\")\n",
    "print(\"Artifacts are clean, structured, and ready for modeling stages.\")\n",
    "print(\"=\" * 90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fba4a5-808a-493a-83f6-57c40c2ecbb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
